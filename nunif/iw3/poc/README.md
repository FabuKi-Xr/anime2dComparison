C, D, L, R are 256px cropped images from the original image (CC0 images).

- C: Input image
- D: Depth image, raw 16bit image generated of [ZoeDepth](https://github.com/isl-org/ZoeDepth) output.
- L, R: left and right eye image generated by `apply_stereo_divergence_polylines` algorithm in  [stable-diffusion-webui-depthmap-script](https://github.com/thygate/stable-diffusion-webui-depthmap-script)

The results of `apply_stereo_divergence_polylines` is great, but it is very slow for video processing.
It takes more than 5 times the processing time than ZoeDepth in my env.

The first step is to make this faster.

I will attempt two alternative algorithms.

- `grid_sample`: A stable naive implementation. It is super ultra very fast. However, ghost artifacts may occur at the edge areas of the depth. 
- distillation model: Train the results of `apply_stereo_divergence_polylines` with ML model using `F.grid_sample`

The dataset used in the ML model requires the following.

- small images randomly crop from the original image
- global maximum and minimum depth values to normalize depth
- width size of original image to apply stereo divergence
- other parameters

They can be stored in PNG metadata.
